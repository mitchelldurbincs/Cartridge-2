# Cartridge2 Configuration
#
# This is the single source of truth for all configuration values.
# All components (actor, trainer, web server) read from this file.
#
# Environment variables can override any value using the pattern:
#   CARTRIDGE_<SECTION>_<KEY> (e.g., CARTRIDGE_TRAINING_ITERATIONS=50)
#
# For Docker, mount this file to /app/config.toml

# =============================================================================
# Common Settings
# =============================================================================
[common]
# Base data directory for all runtime files (replay.db, models/, stats.json)
data_dir = "./data"

# Game environment: tictactoe, connect4
env_id = "connect4"

# Logging level: trace, debug, info, warn, error
log_level = "info"

# =============================================================================
# Training Loop Settings (AlphaZero synchronized training)
# =============================================================================
[training]
# Number of training iterations
iterations = 400 

# Starting iteration number (for resuming training)
start_iteration = 1

# Self-play episodes to generate per iteration
episodes_per_iteration = 500

# Training steps per iteration
# Reduced from 600 to avoid overfitting within each iteration
# (500 episodes × ~25 moves = 12,500 transitions; 400 × 128 = 51,200 samples ≈ 4 epochs)
steps_per_iteration = 400

# Training batch size
batch_size = 128

# Learning rate
learning_rate = 0.001

# Weight decay for regularization
weight_decay = 0.0001

# Gradient clipping max norm (0 to disable)
grad_clip_norm = 1.0

# Device for training: cpu, cuda, mps
device = "cpu"

# Steps between checkpoint saves
checkpoint_interval = 250 

# Maximum number of checkpoints to keep
max_checkpoints = 10

# Number of parallel actor processes for self-play
# More actors = faster episode generation (scales well with CPU cores)
num_actors = 3

# =============================================================================
# Evaluation Settings
# =============================================================================
[evaluation]
# Evaluate every N iterations (0 to disable)
interval = 5

# Number of games per evaluation
games = 200

# Win rate threshold to become new "best" model (gatekeeper)
# Current model must win > this rate against best to replace it
win_threshold = 0.55

# Also evaluate against random baseline (useful for absolute performance)
eval_vs_random = true

# =============================================================================
# Actor Settings (Self-play episode runner)
# =============================================================================
[actor]
# Unique actor identifier
actor_id = "actor-1"

# Maximum episodes to run (-1 for unlimited)
max_episodes = -1

# Timeout per episode in seconds (Connect4 games can take 30+ moves)
episode_timeout_secs = 90

# Interval to flush data to database in seconds
flush_interval_secs = 5

# Log progress every N episodes (0 to disable)
log_interval = 50

# =============================================================================
# Web Server Settings
# =============================================================================
[web]
# Server bind address
host = "0.0.0.0"

# Server port
port = 8080

# =============================================================================
# MCTS Settings (Monte Carlo Tree Search)
# =============================================================================
[mcts]
# MCTS simulation ramping: starts low and increases over iterations
# Formula: start_sims + (iteration-1) * sim_ramp_rate, capped at max_sims
# This speeds up early training when the network is still weak
start_sims = 50       # Simulations for first iteration
max_sims = 400        # Maximum simulations (reached after ramping)
sim_ramp_rate = 20    # Simulations to add per iteration
# With these settings: iter 1=50, iter 10=230, iter 18+=400

# Legacy setting (used if ramping not configured)
num_simulations = 200

# Exploration constant (c_puct)
# Lower values = more exploitation of current best moves
c_puct = 1.0

# Temperature for action selection (higher = more exploration)
temperature = 1.0

# Move number after which to reduce temperature (0 = disabled)
# For Connect4, use ~50% of typical game length for late-game exploitation
temp_threshold = 15

# Dirichlet noise alpha (0 to disable)
# Lower values = more concentrated noise; ~0.3-0.5 good for 7 actions
dirichlet_alpha = 0.4

# Dirichlet noise weight
dirichlet_weight = 0.25

# Batch size for neural network evaluation during MCTS
# Leaves are collected until this batch size, then evaluated together
# Higher values improve throughput but increase latency per batch
# Set to 1 to disable batching (original sequential behavior)
eval_batch_size = 32

# =============================================================================
# Storage Backend Settings
# =============================================================================
[storage]
# Model storage backend: filesystem (local), s3 (K8s)
# For S3, set CARTRIDGE_S3_BUCKET and optionally CARTRIDGE_S3_ENDPOINT
model_backend = "filesystem"

# PostgreSQL connection string (required for replay buffer)
# Can also be set via CARTRIDGE_STORAGE_POSTGRES_URL environment variable
# For local dev: docker compose up postgres
postgres_url = "postgresql://cartridge:cartridge@localhost:5432/cartridge"

# Connection pool settings
pool_max_size = 16          # Maximum connections in the pool
pool_connect_timeout = 30   # Seconds to wait for a connection
pool_idle_timeout = 300     # Seconds before idle connection is closed

# S3 bucket for model storage (only used when model_backend = "s3")
# Can also be set via CARTRIDGE_S3_BUCKET environment variable
# s3_bucket = "cartridge-models"

# S3-compatible endpoint URL (for MinIO, etc.)
# Can also be set via CARTRIDGE_S3_ENDPOINT environment variable
# s3_endpoint = "http://minio:9000"
