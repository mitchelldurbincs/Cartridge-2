# Actor Deployment - Scalable self-play episode generation
#
# This is the horizontally scalable component. Each replica runs independent
# self-play games using MCTS and writes transitions to the shared PostgreSQL
# replay buffer.
#
# Scale with:
#   kubectl scale deployment actor --replicas=8 -n cartridge
#
# Or use HPA for auto-scaling based on CPU/memory.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: actor
  namespace: cartridge
  labels:
    app.kubernetes.io/name: actor
    app.kubernetes.io/component: selfplay
    app.kubernetes.io/part-of: cartridge2
spec:
  # Start with 4 replicas - this is where parallelism happens!
  replicas: 4
  selector:
    matchLabels:
      app.kubernetes.io/name: actor
  template:
    metadata:
      labels:
        app.kubernetes.io/name: actor
        app.kubernetes.io/component: selfplay
        app.kubernetes.io/part-of: cartridge2
    spec:
      initContainers:
        # Wait for PostgreSQL to be ready
        - name: wait-for-postgres
          image: busybox:1.36
          command:
            - sh
            - -c
            - |
              echo "Waiting for PostgreSQL..."
              until nc -z postgres 5432; do
                echo "PostgreSQL not ready, waiting..."
                sleep 2
              done
              echo "PostgreSQL is ready!"
      containers:
        - name: actor
          # Build with: docker build -f Dockerfile.alphazero -t cartridge-actor .
          # For K8s, push to your registry:
          #   docker tag cartridge-actor:latest your-registry/cartridge-actor:latest
          #   docker push your-registry/cartridge-actor:latest
          image: cartridge-actor:latest
          imagePullPolicy: IfNotPresent
          # Override entrypoint to run actor directly (not orchestrator)
          command: ["/app/actor"]
          args:
            - "--env-id=$(ENV_ID)"
            - "--max-episodes=-1"  # Run forever
            - "--data-dir=/data"
            - "--log-interval=$(ACTOR_LOG_INTERVAL)"
            - "--num-simulations=$(MCTS_MAX_SIMS)"
            - "--temp-threshold=$(MCTS_TEMP_THRESHOLD)"
            # Don't use --no-watch so actors hot-reload models
          env:
            - name: ENV_ID
              valueFrom:
                configMapKeyRef:
                  name: cartridge-config
                  key: ENV_ID
            - name: ACTOR_LOG_INTERVAL
              valueFrom:
                configMapKeyRef:
                  name: cartridge-config
                  key: ACTOR_LOG_INTERVAL
            - name: MCTS_MAX_SIMS
              valueFrom:
                configMapKeyRef:
                  name: cartridge-config
                  key: MCTS_MAX_SIMS
            - name: MCTS_TEMP_THRESHOLD
              valueFrom:
                configMapKeyRef:
                  name: cartridge-config
                  key: MCTS_TEMP_THRESHOLD
            - name: CARTRIDGE_STORAGE_POSTGRES_URL
              valueFrom:
                secretKeyRef:
                  name: postgres-credentials
                  key: POSTGRES_URL
            - name: RUST_LOG
              valueFrom:
                configMapKeyRef:
                  name: cartridge-config
                  key: LOG_LEVEL
            # Generate unique actor ID from pod name
            - name: ACTOR_ID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          volumeMounts:
            - name: models
              mountPath: /data/models
              readOnly: true  # Actors only read models
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
          # No liveness/readiness probes since actor doesn't expose HTTP
          # Could add a simple file-based probe if needed
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: models-shared
      # Spread actors across nodes for better resource utilization
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: actor
---
# Optional: Horizontal Pod Autoscaler for actors
# Scales based on CPU utilization (MCTS is CPU-bound)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: actor-hpa
  namespace: cartridge
  labels:
    app.kubernetes.io/name: actor
    app.kubernetes.io/part-of: cartridge2
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: actor
  minReplicas: 2
  maxReplicas: 16
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 2
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 120
